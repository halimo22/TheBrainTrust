{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":98188,"databundleVersionId":12673416,"sourceType":"competition"},{"sourceId":433731,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":353653,"modelId":374958}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torcheeg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch-scatter\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --quiet mne scipy                  # CSP helper tools\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Standard library & basic third‚Äëparty imports\nimport os\nimport argparse\nfrom typing import Optional, Dict, List\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# TorchEEG\nimport numpy as np\nfrom scipy.signal import butter, filtfilt\n\nfrom torcheeg.models import CSPNet\nfrom tqdm.auto import tqdm  # ‚Üê progress bars","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-14T22:54:06.748743Z","iopub.execute_input":"2025-06-14T22:54:06.749321Z","iopub.status.idle":"2025-06-14T22:54:10.704231Z","shell.execute_reply.started":"2025-06-14T22:54:06.749299Z","shell.execute_reply":"2025-06-14T22:54:10.703411Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class ButterBandpass:\n    \"\"\"Callable transform: band‚Äëpass 8‚Äì32‚ÄØHz for each (C,¬†T) EEG array.\n    Ensures *float32* output so tensors match the model weights.\n    \"\"\"\n    def __init__(self, low=8, high=32, fs=250, order=4):\n        self.b, self.a = butter(order, [low, high], btype=\"bandpass\", fs=fs)\n\n    def __call__(self, arr):  # arr shape (8,¬†T)\n        filtered = filtfilt(self.b, self.a, arr, axis=-1,\n                            padlen=3 * max(len(self.a), len(self.b)))\n        return filtered.astype(np.float32, copy=False)\n\n\nclass SegReconstruct:\n    \"\"\"Segmentation & Reconstruction (S&R) augmentation from EEG‚ÄëConformer.\n\n    Splits each trial into *n_seg* equal chunks along the time axis, shuffles\n    them, then re‚Äëconcatenates.  Applied with probability *p*.\n    \"\"\"\n    def __init__(self, n_seg: int = 4, p: float = 0.5):\n        assert 0 < p <= 1, \"p must be in (0,1]\"\n        assert n_seg > 1, \"n_seg must be > 1\"\n        self.n_seg, self.p = n_seg, p\n    def __call__(self, arr):  # arr: (8, T)\n        if np.random.rand() > self.p:\n            return arr.astype(np.float32, copy=False)\n        C, T = arr.shape\n        seg_len = T // self.n_seg\n        idxs = np.arange(self.n_seg)\n        np.random.shuffle(idxs)\n        segments = [arr[:, i*seg_len:(i+1)*seg_len] for i in idxs]\n        shuffled = np.concatenate(segments, axis=-1)\n        # Append remainder if T isn't divisible by n_seg\n        if seg_len * self.n_seg < T:\n            shuffled = np.concatenate([shuffled, arr[:, seg_len*self.n_seg:]], axis=-1)\n        return shuffled.astype(np.float32, copy=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T22:54:10.705393Z","iopub.execute_input":"2025-06-14T22:54:10.706195Z","iopub.status.idle":"2025-06-14T22:54:10.713728Z","shell.execute_reply.started":"2025-06-14T22:54:10.706167Z","shell.execute_reply":"2025-06-14T22:54:10.712962Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\n# -------------------------------------------------\n# 1. Imports\n# -------------------------------------------------\nimport os\nfrom typing import Optional, Dict, List\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torcheeg.models import CSPNet\n\n# %% [markdown]\n\"\"\"\n## 2. Dataset Class\nMaps the Kaggle index CSVs to raw `EEGdata.csv` recordings and slices the requested trial.\n\"\"\"\n\n# %%\nclass MTCBCIDataset(Dataset):\n    \"\"\"PyTorch Dataset for the MTC‚ÄëAIC3 BCI Competition.\n\n    Handles both `S4` and `4` style subject IDs and caches session files to\n    minimise disk I/O.\n    \"\"\"\n\n    EEG_CHANNELS: List[str] = [\n        \"FZ\", \"C3\", \"CZ\", \"C4\", \"PZ\", \"PO7\", \"OZ\", \"PO8\",\n    ]\n\n    LABEL_MAP: Dict[str, Dict[str, int]] = {\n        \"MI\": {\"Left\": 0, \"Right\": 1},\n        \"SSVEP\": {\"Left\": 0, \"Right\": 1, \"Forward\": 2, \"Backward\": 3},\n    }\n\n    def __init__(\n        self,\n        csv_path: str,\n        root_dir: str,\n        cache_eeg: bool = True,\n        transform: Optional[callable] = None,\n    ) -> None:\n        super().__init__()\n        self.df = pd.read_csv(csv_path)\n        self.root_dir = root_dir.rstrip(\"/\\\\\")  # ‚Üê FIXED back‚Äëslash escape\n        self.cache_eeg = cache_eeg\n        self.transform = transform\n        self._eeg_cache: Dict[str, pd.DataFrame] = {}\n\n    # ------------------------------------------------------------------\n    # Dataset API\n    # ------------------------------------------------------------------\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx: int):\n        row = self.df.iloc[idx]\n        task = row[\"task\"]\n        split = self._split_by_id(row[\"id\"])\n\n        # --- subject directory: avoid double \"S\" ---\n        subj_field = str(row[\"subject_id\"])\n        subject_dir = subj_field if subj_field.upper().startswith(\"S\") else f\"S{subj_field}\"\n\n        eeg_csv = os.path.join(\n            self.root_dir,\n            task,\n            split,\n            subject_dir,\n            str(row[\"trial_session\"]),\n            \"EEGdata.csv\",\n        )\n\n        # Load / cache CSV\n        if self.cache_eeg and eeg_csv in self._eeg_cache:\n            eeg_df = self._eeg_cache[eeg_csv]\n        else:\n            eeg_df = pd.read_csv(eeg_csv)\n            if self.cache_eeg:\n                self._eeg_cache[eeg_csv] = eeg_df\n\n        # Slice out the requested trial\n        samples_per_trial = 2250 if task == \"MI\" else 1750\n        start = (int(row[\"trial\"]) - 1) * samples_per_trial\n        end = start + samples_per_trial\n\n        seg = eeg_df.loc[start:end - 1, self.EEG_CHANNELS].to_numpy(np.float32).T  # (8, T)\n        if self.transform:\n            seg = self.transform(seg)\n\n        x = torch.from_numpy(seg.copy()).unsqueeze(0)  # ensure positive strides\n        \n        # Label handling (‚Äë1 for unlabeled test rows)\n        if \"label\" in row and not pd.isna(row[\"label\"]):\n            y = torch.tensor(self.LABEL_MAP[task][row[\"label\"]], dtype=torch.long)\n        else:\n            y = torch.tensor(-1, dtype=torch.long)\n        return x, y\n\n    # ------------------------------------------------------------------\n    # Helper\n    # ------------------------------------------------------------------\n    @staticmethod\n    def _split_by_id(idx: int) -> str:\n        if idx <= 4800:\n            return \"train\"\n        elif idx <= 4900:\n            return \"validation\"\n        return \"test\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T22:54:32.919616Z","iopub.execute_input":"2025-06-14T22:54:32.919941Z","iopub.status.idle":"2025-06-14T22:54:32.931344Z","shell.execute_reply.started":"2025-06-14T22:54:32.919916Z","shell.execute_reply":"2025-06-14T22:54:32.930280Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nimport random\nimport torch\n\ndef sr_collate(batch, n_seg=4, p=0.5):\n    \"\"\"\n    EEG-Conformer Segmentation & Reconstruction (S&R) implementation.\n\n    Parameters\n    ----------\n    batch : list of (x, y) tuples from the Dataset.\n        x shape  = (1, 8, T)\n        y scalar = label (torch.long)\n    n_seg : int\n        Number of equal segments per trial (paper uses 4).\n    p : float\n        Probability of performing S&R augmentation for *this* batch.\n\n    Returns\n    -------\n    xs, ys : torch.Tensor\n        xs shape = (N (+synthetics), 1, 8, T)\n        ys shape = (N (+synthetics),)\n    \"\"\"\n    xs, ys = list(zip(*batch))          # tuples -> lists\n    xs, ys = list(xs), list(ys)\n\n    if random.random() < p:             # decide whether to augment\n        # --- group indices by class ---\n        by_class = {}\n        for idx, y in enumerate(ys):\n            cls = int(y)\n            by_class.setdefault(cls, []).append(idx)\n\n        synth_x, synth_y = [], []\n        for cls, idxs in by_class.items():\n            if len(idxs) < 2:           # need ‚â•2 source trials\n                continue\n\n            # assume all trials same T\n            T = xs[idxs[0]].shape[-1]\n            seg_len = T // n_seg\n\n            seg_list = []\n            for i in range(n_seg):\n                src_idx = random.choice(idxs)\n                seg = xs[src_idx][..., i*seg_len:(i+1)*seg_len]  # (1,8,seg_len)\n                seg_list.append(seg)\n\n            synth_trial = torch.cat(seg_list, dim=-1)            # (1,8,T)\n            synth_x.append(synth_trial)\n            synth_y.append(torch.tensor(cls, dtype=torch.long))\n\n        if synth_x:                      # append to original batch\n            xs.extend(synth_x)\n            ys.extend(synth_y)\n\n    return torch.stack(xs, 0), torch.stack(ys, 0)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T22:54:36.419268Z","iopub.execute_input":"2025-06-14T22:54:36.419565Z","iopub.status.idle":"2025-06-14T22:54:36.427256Z","shell.execute_reply.started":"2025-06-14T22:54:36.419541Z","shell.execute_reply":"2025-06-14T22:54:36.426481Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# %%\nroot_dir = \"/kaggle/input/mtcaic3\"  # ‚¨ÖÔ∏è  CHANGE ME\n\ntask = \"MI\"          # \"MI\"  or  \"SSVEP\"\nepochs = 200\nbatch_size = 128\n\n# %% [markdown]\n\"\"\"\n## 4. Data Preparation\nCreates the training & validation loaders for the chosen task.\n\"\"\"\n# --- Band‚Äëpass transform (8‚Äë32‚ÄØHz, 4th‚Äëorder Butterworth) ---\nbandpass = ButterBandpass(low=8, high=32, fs=250, order=4)\nseg_aug  = SegReconstruct(n_seg=4, p=0.5)          # 50 % chance\ndef combined(arr):\n    arr = bandpass(arr)\n    # arr = seg_aug(arr)\n    return arr\n\n# %%\ntrain_csv = os.path.join(root_dir, \"train.csv\")\nval_csv = os.path.join(root_dir, \"validation.csv\")\n\ntrain_full = MTCBCIDataset(train_csv, root_dir, transform=combined)\nval_full = MTCBCIDataset(val_csv, root_dir, cache_eeg=False)\n\n# train_full = MTCBCIDataset(train_csv, root_dir)\n# val_full = MTCBCIDataset(val_csv, root_dir, cache_eeg=False)\n\n# List of duplicate indices you provided\nduplicates_indices = [\n    1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509,\n    1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659,\n    2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299,\n    2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309\n]\n\n\ntrain_indices = [i for i, r in enumerate(train_full.df.itertuples()) if r.task == task]\nclean_train_indices = [idx for idx in train_indices if idx not in duplicates_indices]\n\nval_indices = [i for i, r in enumerate(val_full.df.itertuples()) if r.task == task]\n\n# train_ds = torch.utils.data.Subset(train_full, train_indices)\nclean_train_ds = torch.utils.data.Subset(train_full, clean_train_indices)\n\nval_ds = torch.utils.data.Subset(val_full, val_indices)\n\nloader_train = DataLoader(clean_train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, collate_fn=lambda b: sr_collate(b, n_seg=5, p=0.5))\nloader_val = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n\n# loader_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n# loader_val = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n\nprint(f\"Loaded {len(clean_train_ds)} training and {len(val_ds)} validation trials for task {task}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T22:55:20.141066Z","iopub.execute_input":"2025-06-14T22:55:20.142057Z","iopub.status.idle":"2025-06-14T22:55:20.169528Z","shell.execute_reply.started":"2025-06-14T22:55:20.142017Z","shell.execute_reply":"2025-06-14T22:55:20.168825Z"}},"outputs":[{"name":"stdout","text":"Loaded 2360 training and 50 validation trials for task MI.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ==============================================================\n# ‚ú¶  CELL  A  ‚ú¶  -- compute analytic CSP filters for the MI task\n# ==============================================================\n\nimport numpy as np, torch, scipy.linalg as la\nfrom tqdm.auto import tqdm\n\nNUM_CSP  = 8                   # f in the paper (use an even number)\nN_CH     = 8                   # electrodes: FZ,C3,CZ,C4,PZ,PO7,OZ,PO8\n\ndef compute_csp_filters(dataloader, n_filters=NUM_CSP):\n    \"\"\"\n    Returns an (n_filters, N_CH) numpy array.\n    Uses the one‚Äëvs‚Äëone CSP solution for 2‚Äëclass MI.\n    \"\"\"\n    cov_sum = {0: np.zeros((N_CH, N_CH)),\n               1: np.zeros((N_CH, N_CH))}\n    n_trial = {0: 0, 1: 0}\n\n    for x, y in tqdm(dataloader, desc=\"CSP|cov\", leave=False):\n        # x : (B, 1, 8, T)\n        x = x.squeeze(1).numpy()                       # (B,8,T)\n        y = y.numpy()\n        for trial, label in zip(x, y):\n            trial = trial - trial.mean(axis=1, keepdims=True)\n            cov   = trial @ trial.T\n            cov   = cov / np.trace(cov)                # normalise\n            cov_sum[label] += cov\n            n_trial[label] += 1\n\n    C0 = cov_sum[0] / n_trial[0]\n    C1 = cov_sum[1] / n_trial[1]\n    Cc = C0 + C1                                       # composite\n\n    # Generalised eigen‚Äëdecomposition  C0‚ÄØw = Œª‚ÄØCc‚ÄØw\n    eigvals, eigvecs = la.eigh(C0, Cc)\n    order = np.argsort(eigvals)[::-1]                  # descending\n    eigvecs = eigvecs[:, order]\n\n    # First k/2 & last k/2 vectors maximise variance for class 0 and 1\n    k = n_filters // 2\n    filters = np.concatenate([eigvecs[:, :k],\n                              eigvecs[:, -k:]], axis=1).T\n    return filters.astype(np.float32)                  # (k*2, 8)\n\n# --------------------------------------------------------------\n# # ‚ö†  Use *exactly* the MI rows of the *training* split\n# # --------------------------------------------------------------\n# train_mi_loader = DataLoader(train_ds, batch_size=64,\n#                              shuffle=False, num_workers=0)\n\ncsp_filters = compute_csp_filters(loader_train, NUM_CSP)\nprint(\"‚úì CSP filters shape:\", csp_filters.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T22:55:26.950715Z","iopub.execute_input":"2025-06-14T22:55:26.951305Z","iopub.status.idle":"2025-06-14T22:55:48.625850Z","shell.execute_reply.started":"2025-06-14T22:55:26.951281Z","shell.execute_reply":"2025-06-14T22:55:48.624976Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"CSP|cov:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"‚úì CSP filters shape: (8, 8)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ==============================================================\n# ‚ú¶  CELL  B  ‚ú¶  -- model definition (faithful CSP‚ÄëNet‚Äë1)\n# ==============================================================\n\nimport torch.nn as nn\nfrom torcheeg.models import EEGNet\nfrom huggingface_hub import hf_hub_download\n\nclass CSPNet1_EEGNet(nn.Module):\n    \"\"\"\n    CSP‚ÄëNet‚Äë1 with EEGNet‚Äëv4 backbone.\n    Args\n    ----\n    csp_w      : ndarray (f, 8)   CSP filters you just computed.\n    freeze_csp : bool            CSP‚ÄëNet‚Äë1‚Äëfix (True) or ‚Äëupd (False).\n    \"\"\"\n    def __init__(self, csp_w, freeze_csp=True,\n                 hf_variant=\"EEGNetv4_BNCI2014004\"):\n        super().__init__()\n        f, C = csp_w.shape                      # (8, 8)\n\n        # 1. CSP projection layer  (B,1,C,T) ‚ûú (B,f,1,T)\n        self.csp = nn.Conv2d(1, f,\n                             kernel_size=(C, 1),\n                             bias=False)\n        with torch.no_grad():\n            self.csp.weight.copy_(\n                torch.tensor(csp_w)             # (f,C)\n                      .unsqueeze(1)             # (f,1,C)\n                      .unsqueeze(-1)            # (f,1,C,1)\n            )\n        if freeze_csp:\n            for p in self.csp.parameters():\n                p.requires_grad = False\n\n        # 2. Complete EEGNet‚Äëv4 backbone\n        self.eegnet = EEGNet(chunk_size=chunk_size,\n                             num_electrodes=f,\n                             num_classes=num_classes)\n\n        # ---- optional HF backbone weights (spatial dims unchanged) ----\n        load_hf_eegnet_weights(self.eegnet, hf_variant)\n\n\n    # ------------------------------------------------------------------\n    def forward(self, x):                         # x: (B,1,8,T)\n        x = self.csp(x)                           # (B,f,1,T)\n        x = x.permute(0, 2, 1, 3).contiguous()    # (B,1,f,T)\n        return self.eegnet(x)\n\n\n# ------------------------------------------------------------------\n# Helper: load compatible HF weights into a torcheeg‚ÄëEEGNet model\n# ------------------------------------------------------------------\ndef load_hf_eegnet_weights(model: torch.nn.Module,\n                           hf_variant: str = \"EEGNetv4_BNCI2014004\") -> None:\n    \"\"\"\n    Copy as many parameters as possible from PierreGtch/EEGNetv4\n    into a torcheeg.models.EEGNet instance whose electrode\n    dimension may differ (8 vs 22).\n    \"\"\"\n    try:\n        ckpt_path = hf_hub_download(\n            repo_id=\"PierreGtch/EEGNetv4\",\n            filename=f\"{hf_variant}/model-params.pkl\",\n            repo_type=\"model\")\n        raw = torch.load(ckpt_path, map_location=\"cpu\")\n    except Exception as e:\n        print(\"‚ö† HF download failed ‚Äî using random init.\\n\", e)\n        return\n\n    # --- map HF names ‚ûú torcheeg names ---------------------------------\n    rename = {\n        \"conv_temporal.weight\"          : \"block1.0.weight\",\n        \"bnorm_temporal.weight\"         : \"block1.1.weight\",\n        \"bnorm_temporal.bias\"           : \"block1.1.bias\",\n        \"bnorm_temporal.running_mean\"   : \"block1.1.running_mean\",\n        \"bnorm_temporal.running_var\"    : \"block1.1.running_var\",\n        \"conv_spatial.weight\"           : \"block1.2.weight\",\n        \"bnorm_1.weight\"                : \"block1.3.weight\",\n        \"bnorm_1.bias\"                  : \"block1.3.bias\",\n        \"bnorm_1.running_mean\"          : \"block1.3.running_mean\",\n        \"bnorm_1.running_var\"           : \"block1.3.running_var\",\n        \"conv_separable_depth.weight\"   : \"block2.0.weight\",\n        \"conv_separable_point.weight\"   : \"block2.1.weight\",\n        \"bnorm_2.weight\"                : \"block2.2.weight\",\n        \"bnorm_2.bias\"                  : \"block2.2.bias\",\n        \"bnorm_2.running_mean\"          : \"block2.2.running_mean\",\n        \"bnorm_2.running_var\"           : \"block2.2.running_var\",\n        # classifier layers (‚Äúconv_classifier.*‚Äù, ‚Äúlin.weight‚Äù) are skipped\n    }\n\n    tgt = model.state_dict()\n    copied = {}\n    for src_name, dst_name in rename.items():\n        if src_name not in raw or dst_name not in tgt:\n            continue\n        if raw[src_name].shape != tgt[dst_name].shape:\n            # conv_spatial.weight is (16,1,22,1) in HF, but (16,1,8,1) here\n            continue\n        copied[dst_name] = raw[src_name]\n\n    tgt.update(copied)\n    model.load_state_dict(tgt, strict=False)\n    print(f\"‚úì Copied {len(copied)}/{len(rename)} compatible tensors \"\n          f\"from HF checkpoint\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T22:55:51.422604Z","iopub.execute_input":"2025-06-14T22:55:51.423222Z","iopub.status.idle":"2025-06-14T22:55:51.585457Z","shell.execute_reply.started":"2025-06-14T22:55:51.423194Z","shell.execute_reply":"2025-06-14T22:55:51.584904Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# 4. Train / eval helpers -------------------------------------------------\ndef train_one_epoch(model, loader, epoch: int):\n    model.train()\n    total, processed = 0.0, 0\n    bar = tqdm(loader, desc=f\"Epoch {epoch} [train]\", leave=False)\n    for x, y in bar:\n        x, y = x.to(device), y.to(device)\n\n        logits = model(x)\n        loss = criterion(logits, y)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n        total += loss.item() * x.size(0)\n        processed += x.size(0)\n        bar.set_postfix(loss=f\"{total/processed:.4f}\")\n\n    return total / processed\n\n\n@torch.no_grad()\ndef evaluate(model, loader, epoch: int):\n    model.eval()\n    preds, labels = [], []\n    bar = tqdm(loader, desc=f\"Epoch {epoch} [val]\", leave=False)\n    for x, y in bar:\n        logits = model(x.to(device))\n        preds.extend(logits.argmax(1).cpu().tolist())\n        labels.extend(y.tolist())\n\n    return (\n        accuracy_score(labels, preds),\n        balanced_accuracy_score(labels, preds),\n        f1_score(labels, preds, average=\"weighted\"),\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T22:55:54.811974Z","iopub.execute_input":"2025-06-14T22:55:54.812268Z","iopub.status.idle":"2025-06-14T22:55:54.818829Z","shell.execute_reply.started":"2025-06-14T22:55:54.812243Z","shell.execute_reply":"2025-06-14T22:55:54.818051Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ==============================================================\n# ‚ú¶  CELL  C  ‚ú¶  -- set up training just like before\n# ==============================================================\n\nFREEZE_CSP   = False        # CSP‚ÄëNet‚Äë1‚Äëfix; set False for ‚Äëupd variant\nLEARNING_RATE = 0.001\nEPOCHS        = 200\nSAVE_PATH     = \"cspnet1_eegnet_best.pth\"\nchunk_size  = 2250 if task == \"MI\" else 1750\nnum_classes = 2    if task == \"MI\" else 4\ndevice = \"cuda\"\nmodel = CSPNet1_EEGNet(csp_filters, freeze_csp=FREEZE_CSP).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,\n                                    model.parameters()),\n                             lr=LEARNING_RATE, weight_decay=5e-4)\n\nprint(model)   # sanity‚Äëcheck: ~0.63‚ÄØM parameters if CSP frozen\n\nbest_bal = 0.0\nfor epoch in range(1, EPOCHS + 1):\n    train_loss = train_one_epoch(model, loader_train, epoch)\n    acc, bal, f1 = evaluate(model, loader_val, epoch)\n    print(f\"Ep {epoch:03d} | loss {train_loss:.4f} | acc {acc:.3f} | bal {bal:.3f}\")\n    if bal > best_bal:\n        best_bal = bal\n        torch.save(model.state_dict(), SAVE_PATH)\n        print(f\"  üöÄ saved new best (bal={best_bal:.3f})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T22:55:57.351990Z","iopub.execute_input":"2025-06-14T22:55:57.352677Z","iopub.status.idle":"2025-06-14T22:56:39.017357Z","shell.execute_reply.started":"2025-06-14T22:55:57.352622Z","shell.execute_reply":"2025-06-14T22:56:39.016173Z"}},"outputs":[{"name":"stdout","text":"‚úì Copied 15/16 compatible tensors from HF checkpoint\nCSPNet1_EEGNet(\n  (csp): Conv2d(1, 8, kernel_size=(8, 1), stride=(1, 1), bias=False)\n  (eegnet): EEGNet(\n    (block1): Sequential(\n      (0): Conv2d(1, 8, kernel_size=(1, 64), stride=(1, 1), padding=(0, 32), bias=False)\n      (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n      (2): Conv2dWithConstraint(8, 16, kernel_size=(8, 1), stride=(1, 1), groups=8, bias=False)\n      (3): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n      (4): ELU(alpha=1.0)\n      (5): AvgPool2d(kernel_size=(1, 4), stride=4, padding=0)\n      (6): Dropout(p=0.25, inplace=False)\n    )\n    (block2): Sequential(\n      (0): Conv2d(16, 16, kernel_size=(1, 16), stride=(1, 1), padding=(0, 8), groups=16, bias=False)\n      (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (2): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n      (3): ELU(alpha=1.0)\n      (4): AvgPool2d(kernel_size=(1, 8), stride=8, padding=0)\n      (5): Dropout(p=0.25, inplace=False)\n    )\n    (lin): Linear(in_features=1120, out_features=2, bias=False)\n  )\n)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1 [train]:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1 [val]:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Ep 001 | loss 0.6999 | acc 0.560 | bal 0.500\n  üöÄ saved new best (bal=0.500)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2 [train]:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 2 [val]:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Ep 002 | loss 0.6917 | acc 0.560 | bal 0.500\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3 [train]:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 3 [val]:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Ep 003 | loss 0.6875 | acc 0.480 | bal 0.492\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4 [train]:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 4 [val]:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Ep 004 | loss 0.6864 | acc 0.520 | bal 0.484\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5 [train]:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc608b50112143c4bc28a2d41338708d"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_323/3101328536.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mbest_bal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Ep {epoch:03d} | loss {train_loss:.4f} | acc {acc:.3f} | bal {bal:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_323/296070267.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, epoch)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch} [train]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_323/2718655267.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msamples_per_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meeg_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEEG_CHANNELS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m  \u001b[0;31m# (8, T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multi_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple_same_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAxisInt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple_same_dim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1018\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# We should never have retval.ndim < self.ndim, as that should\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0;31m#  be handled by the _getitem_lowerdim call above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1418\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;31m# A collection of keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m         \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1558\u001b[0;31m         \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6243\u001b[0m         \u001b[0;31m# Count missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6244\u001b[0m         \u001b[0mmissing_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6245\u001b[0;31m         \u001b[0mnmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     47\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[1;32m     48\u001b[0m          initial=_NoValue, where=True):\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}